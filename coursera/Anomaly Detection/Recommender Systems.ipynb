{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Formulation\n",
    "\n",
    "Recommendation is currently a very popular application of machine learning.\n",
    "\n",
    "Say we are trying to recommend movies to customers. We can use the following definitions:\n",
    "* \\\\(n_u\\\\) = number of users\n",
    "* \\\\(n_m\\\\) = number of movies\n",
    "* \\\\(r(i,j)\\\\) = 1 if user j has rated movie i\n",
    "* \\\\(y(i,j)\\\\) = rating given by user j to movie i (defined only if r(i,j)=1)\n",
    "\n",
    "\n",
    "## Content Based Recommendations\n",
    "\n",
    "We can introduce two features, x1 and x2 which represents how much romance or how much action a movie may have (on a scale of 0−1).\n",
    "\n",
    "One approach is that we could do linear regression for every single user. For each user j, learn a parameter \\\\(\\theta^{(j)} \\in \\mathbb{R}^3\\\\). Predict user j as rating movie i with \\\\((\\theta^{(j)})^Tx^{(i)}\\\\) stars.\n",
    "\n",
    "* \\\\(\\theta^{(j)} = \\\\) parameter vector for user j\n",
    "* \\\\(x^{(i)} = \\\\) feature vector for movie i\n",
    "\n",
    "For user j, movie i, predicted rating: \\\\((\\theta^{(j)})^T(x^{(i)})\\\\)\n",
    "\n",
    "To learn \\\\(\\theta^{(j)}\\\\), we do the following:\n",
    "\n",
    "$$min_{\\theta^{(j)}} = \\dfrac{1}{2}\\displaystyle \\sum_{i:r(i,j)=1} ((\\theta^{(j)})^T(x^{(i)}) - y^{(i,j)})^2 + \\dfrac{\\lambda}{2} \\sum_{k=1}^n(\\theta_k^{(j)})^2$$\n",
    "\n",
    "This is our familiar linear regression. The base of the first summation is choosing all i such that \\\\(r(i,j) = 1\\\\)\n",
    "\n",
    "To get the parameters for all our users, we do the following:\n",
    "\n",
    "$$min_{\\theta^{(1)},\\dots,\\theta^{(n_u)}} = \\dfrac{1}{2}\\displaystyle \\sum_{j=1}^{n_u}  \\sum_{i:r(i,j)=1} ((\\theta^{(j)})^T(x^{(i)}) - y^{(i,j)})^2 + \\dfrac{\\lambda}{2} \\sum_{j=1}^{n_u} \\sum_{k=1}^n(\\theta_k^{(j)})^2$$\n",
    "\n",
    "We can apply our linear regression gradient descent update using the above cost function.\n",
    "\n",
    "The only real difference is that we **eliminate the constant** \\\\(\\dfrac{1}{m}\\\\).\n",
    "\n",
    "\n",
    "## Collaborative Filtering\n",
    "\n",
    "It can be very difficult to find features such as \"amount of romance\" or \"amount of action\" in a movie. To figure this out, we can use *feature finders*.\n",
    "\n",
    "We can let the users tell us how much they like the different genres, providing their parameter vector immediately for us.\n",
    "\n",
    "To infer the features from given parameters, we use the squared error function with regularization over all the users:\n",
    "\n",
    "$$min_{x^{(1)},\\dots,x^{(n_m)}} \\dfrac{1}{2} \\displaystyle \\sum_{i=1}^{n_m}  \\sum_{j:r(i,j)=1} ((\\theta^{(j)})^T x^{(i)} - y^{(i,j)})^2 + \\dfrac{\\lambda}{2}\\sum_{i=1}^{n_m} \\sum_{k=1}^{n} (x_k^{(i)})^2$$\n",
    "\n",
    "You can also **randomly guess** the values for theta to guess the features repeatedly. You will actually converge to a good set of features.\n",
    "\n",
    "\n",
    "## Collaborative Filtering Algorithm\n",
    "\n",
    "To speed things up, we can simultaneously minimize our features and our parameters:\n",
    "\n",
    "$$J(x,\\theta) = \\dfrac{1}{2} \\displaystyle \\sum_{(i,j):r(i,j)=1}((\\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2 + \\dfrac{\\lambda}{2}\\sum_{i=1}^{n_m} \\sum_{k=1}^{n} (x_k^{(i)})^2 + \\dfrac{\\lambda}{2}\\sum_{j=1}^{n_u} \\sum_{k=1}^{n} (\\theta_k^{(j)})^2$$\n",
    "\n",
    "It looks very complicated, but we've only combined the cost function for theta and the cost function for x.\n",
    "\n",
    "Because the algorithm can learn them itself, the bias units where x0=1 have been removed, therefore x∈ℝn and θ∈ℝn.\n",
    "\n",
    "These are the steps in the algorithm:\n",
    "\n",
    "1. Initialize \\\\(x^{(i)},...,x^{(n_m)},\\theta^{(1)},...,\\theta^{(n_u)}\\\\) to small random values. This serves to break symmetry and ensures that the algorithm learns features \\\\(x^{(i)},...,x^{(n_m)}\\\\) that are different from each other.\n",
    "2. Minimize \\\\(J(x^{(i)},...,x^{(n_m)},\\theta^{(1)},...,\\theta^{(n_u)})\\\\) using gradient descent (or an advanced optimization algorithm).E.g. for every \\\\(j=1,...,n_u,i=1,...n_m\\\\):\n",
    "$$x_k^{(i)} := x_k^{(i)} - \\alpha\\left (\\displaystyle \\sum_{j:r(i,j)=1}{((\\theta^{(j)})^T x^{(i)} - y^{(i,j)}) \\theta_k^{(j)}} + \\lambda x_k^{(i)} \\right)$$\n",
    "3. For a user with parameters θ and a movie with (learned) features x, predict a star rating of \\\\(\\theta^Tx\\\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
