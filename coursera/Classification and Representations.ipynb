{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "To attempt classification, one method is to use linear regression and map all predictions greater than 0.5 as a 1 and all less than 0.5 as a 0. However, this method doesn't work well because classification is not actually a linear function.\n",
    "\n",
    "The classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values. For now, we will focus on the **binary classification problem** in which y can take on only two values, 0 and 1. (Most of what we say here will also generalize to the multiple-class case.) For instance, if we are trying to build a spam classifier for email, then \\\\(x^{(i)}\\\\)  may be some features of a piece of email, and y may be 1 if it is a piece of spam mail, and 0 otherwise. Hence, y∈{0,1}. 0 is also called the negative class, and 1 the positive class, and they are sometimes also denoted by the symbols “-” and “+.” Given \\\\(x^{(i)}\\\\), the corresponding \\\\(y^{(i)}\\\\) is also called the label for the training example.\n",
    "\n",
    "\n",
    "## Hypothesis Representation\n",
    "\n",
    "We could approach the classification problem ignoring the fact that y is discrete-valued, and use our old linear regression algorithm to try to predict y given x. However, it is easy to construct examples where this method performs very poorly. Intuitively, it also doesn’t make sense for \\\\(h_\\theta (x)\\\\)  to take values larger than 1 or smaller than 0 when we know that y ∈ {0, 1}. To fix this, let’s change the form for our hypotheses \\\\(h_\\theta (x)\\\\) to satisfy \\\\(0 \\leq h_\\theta (x) \\leq 1\\\\). This is accomplished by plugging \\\\(\\theta^Tx\\\\) into the Logistic Function.\n",
    "\n",
    "Our new form uses the \"Sigmoid Function,\" also called the \"Logistic Function\":\n",
    "\n",
    "$$\\begin{align*}& h_\\theta (x) = g ( \\theta^T x ) \\newline \\newline& z = \\theta^T x \\newline& g(z) = \\dfrac{1}{1 + e^{-z}}\\end{align*}$$\n",
    "\n",
    "The following image shows us what the sigmoid function looks like: \n",
    "\n",
    "![Logistic_function](img/Logistic_function.png \"Logistic_function\")\n",
    "\n",
    "The function g(z), shown here, maps any real number to the (0, 1) interval, making it useful for transforming an arbitrary-valued function into a function better suited for classification.\n",
    "\n",
    "\\\\(h_\\theta(x)\\\\) will give us the **probability** that our output is 1. For example, \\\\(h_\\theta(x)=0.7\\\\) gives us a probability of 70% that our output is 1. Our probability that our prediction is 0 is just the complement of our probability that it is 1 (e.g. if probability that it is 1 is 70%, then the probability that it is 0 is 30%).\n",
    "\n",
    "$$\\begin{align*}& h_\\theta(x) = P(y=1 | x ; \\theta) = 1 - P(y=0 | x ; \\theta) \\newline& P(y = 0 | x;\\theta) + P(y = 1 | x ; \\theta) = 1\\end{align*}$$\n",
    "\n",
    "\n",
    "## Decision Boundary\n",
    "\n",
    "In order to get our discrete 0 or 1 classification, we can translate the output of the hypothesis function as follows:\n",
    "\n",
    "$$\\begin{align*}& h_\\theta(x) \\geq 0.5 \\rightarrow y = 1 \\newline& h_\\theta(x) < 0.5 \\rightarrow y = 0 \\newline\\end{align*}$$\n",
    "\n",
    "The way our logistic function g behaves is that when its input is greater than or equal to zero, its output is greater than or equal to 0.5:\n",
    "\n",
    "$$\\begin{align*}& g(z) \\geq 0.5 \\newline& when \\; z \\geq 0\\end{align*}$$\n",
    "\n",
    "Remember.\n",
    "\n",
    "$$\\begin{align*}z=0, e^{0}=1 \\Rightarrow g(z)=1/2\\newline z \\to \\infty, e^{-\\infty} \\to 0 \\Rightarrow g(z)=1 \\newline z \\to -\\infty, e^{\\infty}\\to \\infty \\Rightarrow g(z)=0 \\end{align*}$$\n",
    "\n",
    "So if our input to g is \\\\(\\theta^T X\\\\), then that means:\n",
    "\n",
    "$$\\begin{align*}& h_\\theta(x) = g(\\theta^T x) \\geq 0.5 \\newline& when \\; \\theta^T x \\geq 0\\end{align*}$$\n",
    "\n",
    "From these statements we can now say:\n",
    "\n",
    "$$\\begin{align*}& \\theta^T x \\geq 0 \\Rightarrow y = 1 \\newline& \\theta^T x < 0 \\Rightarrow y = 0 \\newline\\end{align*}$$\n",
    "\n",
    "The **decision boundary** is the line that separates the area where y = 0 and where y = 1. It is created by our hypothesis function.\n",
    "\n",
    "Example:\n",
    "\n",
    "$$\\begin{align*}& \\theta = \\begin{bmatrix}5 \\newline -1 \\newline 0\\end{bmatrix} \\newline & y = 1 \\; if \\; 5 + (-1) x_1 + 0 x_2 \\geq 0 \\newline & 5 - x_1 \\geq 0 \\newline & - x_1 \\geq -5 \\newline& x_1 \\leq 5 \\newline \\end{align*}$$\n",
    "\n",
    "to the left of that denotes y = 1, while everything to the right denotes y = 0.\n",
    "\n",
    "Again, the input to the sigmoid function g(z) (e.g. \\\\(\\theta^T X)\\\\) doesn't need to be linear, and could be a function that describes a circle (e.g. \\\\(z = \\theta_0 + \\theta_1 x_1^2 +\\theta_2 x_2^2)\\\\) or any shape to fit our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
