{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Metrics for Skewed Classes\n",
    "\n",
    "It is sometimes difficult to tell whether a reduction in error is actually an improvement of the algorithm.\n",
    "\n",
    "* For example: In predicting a cancer diagnoses where 0.5% of the examples have cancer, we find our learning algorithm has a 1% error. However, if we were to simply classify every single example as a 0, then our error would reduce to 0.5% even though we did not improve the algorithm.\n",
    "\n",
    "This usually happens with **skewed classes**; that is, when our class is very rare in the entire data set.\n",
    "\n",
    "Or to say it another way, when we have lot more examples from one class than from the other class.\n",
    "\n",
    "For this we can use **Precision/Recall**.\n",
    "\n",
    "* Predicted: 1, Actual: 1 --- True positive\n",
    "* Predicted: 0, Actual: 0 --- True negative\n",
    "* Predicted: 0, Actual, 1 --- False negative\n",
    "* Predicted: 1, Actual: 0 --- False positive\n",
    "\n",
    "**Precision**: of all patients we predicted where y=1, what fraction actually has cancer?\n",
    "\n",
    "$$\\dfrac{\\text{True Positives}}{\\text{Total number of predicted positives}} = \\dfrac{\\text{True Positives}}{\\text{True Positives}+\\text{False positives}}$$\n",
    "\n",
    "**Recall**: Of all the patients that actually have cancer, what fraction did we correctly detect as having cancer?\n",
    "\n",
    "$$\\dfrac{\\text{True Positives}}{\\text{Total number of actual positives}}= \\dfrac{\\text{True Positives}}{\\text{True Positives}+\\text{False negatives}}$$\n",
    "\n",
    "These two metrics give us a better sense of how our classifier is doing. We want both precision and recall to be high.\n",
    "\n",
    "In the example at the beginning of the section, if we classify all patients as 0, then our recall will be \\\\(\\dfrac{0}{0 + f} = 0\\\\), so despite having a lower error percentage, we can quickly see it has worse recall.\n",
    "\n",
    "$$\\frac {true positive + true negative} {total population}$$\n",
    "\n",
    "Note 1: if an algorithm predicts only negatives like it does in one of exercises, the precision is not defined, it is impossible to divide by 0. F1 score will not be defined too.\n",
    "\n",
    "\n",
    "## Trading Off Precision and Recall\n",
    "\n",
    "We might want a **confident** prediction of two classes using logistic regression. One way is to increase our threshold:\n",
    "\n",
    "* Predict 1 if: \\\\(h_\\theta(x) \\geq 0.7\\\\)\n",
    "* Predict 0 if: \\\\(h_\\theta(x) < 0.7\\\\)\n",
    "\n",
    "This way, we only predict cancer if the patient has a 70% chance.\n",
    "\n",
    "Doing this, we will have **higher precision** but **lower recall** (refer to the definitions in the previous section).\n",
    "\n",
    "In the opposite example, we can lower our threshold:\n",
    "\n",
    "* Predict 1 if: \\\\(h_\\theta(x) \\geq 0.3\\\\)\n",
    "* Predict 0 if: \\\\(h_\\theta(x) < 0.3\\\\)\n",
    "\n",
    "That way, we get a very **safe** prediction. This will cause **lower precision** but **higher recall**.\n",
    "\n",
    "The greater the threshold, the greater the precision and the lower the recall.\n",
    "\n",
    "The lower the threshold, the greater the recall and the lower the precision.\n",
    "\n",
    "In order to turn these two metrics into one single number, we can take the **F value**.\n",
    "\n",
    "One way is to take the **average**:\n",
    "\n",
    "$$\\dfrac{P+R}{2}$$\n",
    "\n",
    "This does not work well. If we predict all y=0 then that will bring the average up despite having 0 recall. If we predict all examples as y=1, then the very high recall will bring up the average despite having 0 precision.\n",
    "\n",
    "A better way is to compute the **F Score** (or F1 score):\n",
    "\n",
    "$$\\text{F Score} = 2\\dfrac{PR}{P + R}$$\n",
    "\n",
    "In order for the F Score to be large, both precision and recall must be large.\n",
    "\n",
    "We want to train precision and recall on the **cross validation** set so as not to bias our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
