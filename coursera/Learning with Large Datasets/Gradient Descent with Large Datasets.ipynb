{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning with Large Datasets\n",
    "\n",
    "We mainly benefit from a very large dataset when our algorithm has high variance when m is small. Recall that if our algorithm has high bias, more data will not have any benefit.\n",
    "\n",
    "Datasets can often approach such sizes as m = 100,000,000. In this case, our gradient descent step will have to make a summation over all one hundred million examples. We will want to try to avoid this -- the approaches for doing so are described below.\n",
    "\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "Stochastic gradient descent is an alternative to classic (or batch) gradient descent and is more efficient and scalable to large data sets.\n",
    "\n",
    "Stochastic gradient descent is written out in a different but similar way:\n",
    "\n",
    "$$cost(\\theta,(x^{(i)}, y^{(i)})) = \\dfrac{1}{2}(h_{\\theta}(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "The only difference in the above cost function is the elimination of the m constant within \\\\(\\dfrac{1}{2}\\\\).\n",
    "\n",
    "$$J_{train}(\\theta) = \\dfrac{1}{m} \\displaystyle \\sum_{i=1}^m cost(\\theta, (x^{(i)}, y^{(i)}))$$\n",
    "\n",
    "\\\\(J_{train}\\\\) is now just the average of the cost applied to all of our training examples.\n",
    "\n",
    "The algorithm is as follows\n",
    "\n",
    "1. Randomly 'shuffle' the dataset\n",
    "2. For \\\\(i = 1\\dots m\\\\):\n",
    "    $$\\Theta_j := \\Theta_j - \\alpha (h_{\\Theta}(x^{(i)}) - y^{(i)}) \\cdot x^{(i)}_j$$\n",
    "    \n",
    "This algorithm will only try to fit one training example at a time. This way we can make progress in gradient descent without having to scan all m training examples first. Stochastic gradient descent will be unlikely to converge at the global minimum and will instead wander around it randomly, but usually yields a result that is close enough. Stochastic gradient descent will **usually take 1-10 passes** through your data set to get near the global minimum.\n",
    "\n",
    "\n",
    "## Mini-Batch Gradient Descent\n",
    "\n",
    "Mini-batch gradient descent can sometimes be even faster than stochastic gradient descent. Instead of using all m examples as in batch gradient descent, and instead of using only 1 example as in stochastic gradient descent, we will use some in-between number of examples b.\n",
    "\n",
    "Typical values for b range from 2-100 or so.\n",
    "\n",
    "For example, with b=10 and m=1000:<br>\n",
    "Repeat:\n",
    "    \n",
    "> For \\\\(i = 1,11,21,31,\\dots,991\\\\)\n",
    ">> \\\\(\\theta_j := \\theta_j - \\alpha \\dfrac{1}{10} \\displaystyle \\sum_{k=i}^{i+9} (h_\\theta(x^{(k)}) - y^{(k)})x_j^{(k)}\\\\)\n",
    "\n",
    "We're simply summing over ten examples at a time. The advantage of computing more than one example at a time is that we can use vectorized implementations over the b examples.\n",
    "\n",
    "\n",
    "## Stochastic Gradient Descent Convergence\n",
    "\n",
    "How do we choose the learning rate α for stochastic gradient descent? Also, how do we debug stochastic gradient descent to make sure it is getting as close as possible to the global optimum?\n",
    "\n",
    "One strategy is to plot the average cost of the hypothesis applied to every 1000 or so training examples. We can compute and save these costs during the gradient descent iterations.\n",
    "\n",
    "With a smaller learning rate, it is possible that you may get a slightly better solution with stochastic gradient descent. That is because stochastic gradient descent will oscillate and jump around the global minimum, and it will make smaller random jumps with a smaller learning rate.\n",
    "\n",
    "If you increase the number of examples you average over to plot the performance of your algorithm, the plot's line will become smoother.\n",
    "\n",
    "With a very small number of examples for the average, the line will be too noisy and it will be difficult to find the trend.\n",
    "\n",
    "One strategy for trying to actually converge at the global minimum is to slowly decrease α over time. For example:<br>\n",
    "$$\\alpha = \\dfrac{const1}{iterationNumber + const2}$$\n",
    "\n",
    "However, this is not often done because people don't want to have to fiddle with even more parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
