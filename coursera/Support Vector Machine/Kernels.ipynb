{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel 1\n",
    "\n",
    "**Kernels** allow us to make complex, non-linear classifiers using Support Vector Machines.\n",
    "\n",
    "Given x, compute new feature depending on proximity to landmarks \\\\(l^{(1)},\\ l^{(2)},\\ l^{(3)}\\\\).\n",
    "\n",
    "To do this, we find the **similarity** of x and some landmark \\\\(l^{(i)}\\\\):\n",
    "\n",
    "$$f_i = similarity(x, l^{(i)}) = \\exp(-\\dfrac{||x - l^{(i)}||^2}{2\\sigma^2})$$\n",
    "\n",
    "This \"similarity\" function is called a **Gaussian Kernel**. It is a specific example of a kernel.\n",
    "\n",
    "The similarity function can also be written as follows:\n",
    "\n",
    "$$f_i = similarity(x, l^{(i)}) = \\exp(-\\dfrac{\\sum^n_{j=1}(x_j-l_j^{(i)})^2}{2\\sigma^2})$$\n",
    "\n",
    "There are a couple properties of the similarity function:\n",
    "\n",
    "* If \\\\(x \\approx l^{(i)}\\\\), then \\\\(f_i = \\exp(-\\dfrac{\\approx 0^2}{2\\sigma^2}) \\approx 1\\\\).\n",
    "* If x is far from \\\\(l^{(i)}\\\\), then \\\\(f_i = \\exp(-\\dfrac{(large\\ number)^2}{2\\sigma^2}) \\approx 0\\\\).\n",
    "\n",
    "In other words, if x and the landmark are close, then the similarity will be close to 1, and if x and the landmark are far away from each other, the similarity will be close to 0.\n",
    "\n",
    "Each landmark gives us the features in our hypothesis:\n",
    "\n",
    "$$\\begin{align*}l^{(1)} \\rightarrow f_1 \\newline l^{(2)} \\rightarrow f_2 \\newline l^{(3)} \\rightarrow f_3 \\newline\\dots \\newline h_\\Theta(x) = \\Theta_1f_1 + \\Theta_2f_2 + \\Theta_3f_3 + \\dots\\end{align*}$$\n",
    "\n",
    "\\\\(\\sigma^2\\\\) is a parameter of the Gaussian Kernel, and it can be modified to increase or decrease the **drop-off** of our feature \\\\(f_i\\\\). Combined with looking at the values inside Θ, we can choose these landmarks to get the general shape of the decision boundary.\n",
    "\n",
    "![](../img/svm-kernel.png)\n",
    "\n",
    "\n",
    "## Kernel 2\n",
    "\n",
    "One way to get the landmarks is to put them in the **exact same** locations as all the training examples. This gives us m landmarks, with one landmark per training example.\n",
    "\n",
    "Given example x:\n",
    "\n",
    "$$\\begin{align*}f_1 = similarity(x,l^{(1)}) \\newline f_2 = similarity(x,l^{(2)}) \\newline f_3 = similarity(x,l^{(3)}) \\newline ... \\end{align*}$$\n",
    "\n",
    "This gives us a \"feature vector,\" //(f_{(i)}//) of all our features for example //(x_{(i)}//). We may also set //(f_0 = 1//) to correspond with Θ0. Thus given training example //(x_{(i)}//):\n",
    "\n",
    "$$x^{(i)} \\rightarrow \\begin{bmatrix}f_1^{(i)} = similarity(x^{(i)}, l^{(1)}) \\newline f_2^{(i)} = similarity(x^{(i)}, l^{(2)}) \\newline\\vdots \\newline f_m^{(i)} = similarity(x^{(i)}, l^{(m)}) \\newline\\end{bmatrix}$$\n",
    "\n",
    "Now to get the parameters Θ we can use the SVM minimization algorithm but with //(f^{(i)}//) substituted in for //(x^{(i)}//):\n",
    "\n",
    "$$\\min_{\\Theta} C \\sum_{i=1}^m y^{(i)}\\text{cost}_1(\\Theta^Tf^{(i)}) + (1 - y^{(i)})\\text{cost}_0(\\theta^Tf^{(i)}) + \\dfrac{1}{2}\\sum_{j=1}^n \\Theta^2_j$$\n",
    "\n",
    "Using kernels to generate f(i) is not exclusive to SVMs and may also be applied to logistic regression. However, because of computational optimizations on SVMs, kernels combined with SVMs is much faster than with other algorithms, so kernels are almost always found combined only with SVMs.\n",
    "\n",
    "### Choosing SVM Parameters\n",
    "\n",
    "Choosing C (recall that \\\\(C = \\dfrac{1}{\\lambda}\\\\)):\n",
    "\n",
    "* If C is large, then we get higher variance/lower bias\n",
    "* If C is small, then we get lower variance/higher bias\n",
    "\n",
    "The other parameter we must choose is \\\\(σ^2\\\\) from the Gaussian Kernel function\n",
    "\n",
    "* With a large \\\\(σ^2\\\\), the features \\\\(f_i\\\\) vary more smoothly, causing higher bias and lower variance.\n",
    "* With a small \\\\(σ^2\\\\), the features \\\\(f_i\\\\) vary less smoothly, causing lower bias and higher variance.\n",
    "\n",
    "\n",
    "## Using SVM\n",
    "\n",
    "There are lots of good SVM libraries already written. A. Ng often uses **liblinear** and **libsvm**. In practical application, you should use one of these libraries rather than rewrite the functions.\n",
    "\n",
    "In practical application, the choices you do need to make are:\n",
    "\n",
    "* Choice of parameter C\n",
    "* Choice of kernel (similarity function)\n",
    "* Choose when n is large and m is small:<br>\n",
    "    No kernel (\"linear\" kernel) -- gives standard linear classifier\n",
    "* Choose when n is small and m is intermediate:<br>\n",
    "    Gaussian Kernel (above) -- need to choose \\\\(σ^2\\\\)\n",
    "* Choose when n is small and m is large:<br>\n",
    "    No kernel (\"linear\" kernel) -- gives standard linear classifier\n",
    "    \n",
    "The library may ask you to provide the kernel function.\n",
    "\n",
    "**Note**: do perform **feature scaling** before using the **Gaussian Kernel**.\n",
    "\n",
    "**Note**: not all similarity functions are valid kernels. They must satisfy **Mercer's Theorem** which guarantees that the SVM package's optimizations run correctly and do not diverge.\n",
    "\n",
    "You want to train C and the parameters for the kernel function using the training and cross-validation datasets.\n",
    "\n",
    "\n",
    "## Multi-class Classification\n",
    "\n",
    "Many SVM libraries have multi-class classification built-in.\n",
    "\n",
    "You can use the *one-vs-all* method just like we did for logistic regression, where //(y \\in {1,2,3,\\dots,K}//) with //(\\Theta^{(1)}, \\Theta^{(2)}, \\dots,\\Theta{(K)}//). We pick class i with the largest //((\\Theta^{(i)})^Tx//).\n",
    "\n",
    "\n",
    "## Logistic Regression vs. SVMs\n",
    "\n",
    "* If **n is large** (relative to m), then use logistic regression, or SVM without a kernel (the \"linear kernel\")\n",
    "* If **n is small** and **m is intermediate**, then use SVM with a Gaussian Kernel\n",
    "* If **n is small** and **m is large**, then manually create/add more features, then use logistic regression or SVM without a kernel.\n",
    "\n",
    "In the first case, we don't have enough examples to need a complicated polynomial hypothesis. In the second example, we have enough examples that we may need a complex non-linear hypothesis. In the last case, we want to increase our features so that logistic regression becomes applicable.\n",
    "\n",
    "**Note**: a neural network is likely to work well for any of these situations, but may be slower to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
