{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Objective\n",
    "\n",
    "The **Support Vector Machine (SVM)** is yet another type of supervised machine learning algorithm. It is sometimes cleaner and more powerful.\n",
    "\n",
    "Recall that in logistic regression, we use the following rules:\n",
    "\n",
    "if y=1, then \\\\(h_\\theta(x) \\approx 1\\\\) and \\\\(\\Theta^Tx \\gg 0\\\\)\n",
    "\n",
    "if y=0, then \\\\(h_\\theta(x) \\approx 0\\\\) and \\\\(\\Theta^Tx \\ll 0\\\\)\n",
    "\n",
    "Recall the cost function for (unregularized) logistic regression:\n",
    "\n",
    "$$\\begin{align*}J(\\theta) & = \\frac{1}{m}\\sum_{i=1}^m -y^{(i)} \\log(h_\\theta(x^{(i)})) - (1 - y^{(i)})\\log(1 - h_\\theta(x^{(i)}))\\\\ & = \\frac{1}{m}\\sum_{i=1}^m -y^{(i)} \\log\\Big(\\dfrac{1}{1 + e^{-\\theta^Tx^{(i)}}}\\Big) - (1 - y^{(i)})\\log\\Big(1 - \\dfrac{1}{1 + e^{-\\theta^Tx^{(i)}}}\\Big)\\end{align*}$$\n",
    "\n",
    "To make a support vector machine, we will modify the first term of the cost function \\\\(-\\log(h_{\\theta}(x)) = -\\log\\Big(\\dfrac{1}{1 + e^{-\\theta^Tx}}\\Big)\\\\) so that when \\\\(θ^Tx\\\\) (from now on, we shall refer to this as z) is **greater than** 1, it outputs 0. Furthermore, for values of z less than 1, we shall use a straight decreasing line instead of the sigmoid curve.(In the literature, this is called a hinge loss (https://en.wikipedia.org/wiki/Hinge_loss) function.)\n",
    "\n",
    "![](../img/svm_hing.png)\n",
    "\n",
    "Similarly, we modify the second term of the cost function \\\\(-\\log(1 - h_{\\theta(x)}) = -\\log\\Big(1 - \\dfrac{1}{1 + e^{-\\theta^Tx}}\\Big)\\\\) so that when z is **less than** -1, it outputs 0. We also modify it so that for values of z greater than -1, we use a straight increasing line instead of the sigmoid curve.\n",
    "\n",
    "![](../img/svm_hinge_negative_class.png)\n",
    "\n",
    "We shall denote these as \\\\(cost_1(z)\\\\) and \\\\(cost_0(z)\\\\) (respectively, note that \\\\(cost_1(z)\\\\) is the cost for classifying when y=1, and \\\\(cost_0(z)\\\\) is the cost for classifying when y=0), and we may define them as follows (where k is an arbitrary constant defining the magnitude of the slope of the line):\n",
    "\n",
    "$$z = \\theta^Tx$$\n",
    "$$\\text{cost}_0(z) = \\max(0, k(1+z))$$\n",
    "$$\\text{cost}_1(z) = \\max(0, k(1-z))$$\n",
    "\n",
    "Recall the full cost function from (regularized) logistic regression:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m y^{(i)}(-\\log(h_\\theta(x^{(i)}))) + (1 - y^{(i)})(-\\log(1 - h_\\theta(x^{(i)}))) + \\dfrac{\\lambda}{2m}\\sum_{j=1}^n \\Theta^2_j$$\n",
    "\n",
    "Note that the negative sign has been distributed into the sum in the above equation.\n",
    "\n",
    "We may transform this into the cost function for support vector machines by substituting \\\\(cost_1(z)\\\\) and \\\\(cost_0(z)\\\\):\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m y^{(i)} \\ \\text{cost}_1(\\theta^Tx^{(i)}) + (1 - y^{(i)}) \\ \\text{cost}_0(\\theta^Tx^{(i)}) + \\dfrac{\\lambda}{2m}\\sum_{j=1}^n \\Theta^2_j$$\n",
    "\n",
    "We can optimize this a bit by multiplying this by m (thus removing the m factor in the denominators). Note that this does not affect our optimization, since we're simply multiplying our cost function by a positive constant (for example, minimizing \\\\((u-5)^2 + 1\\\\) gives us 5; multiplying it by 10 to make it \\\\(10(u-5)^2 + 10\\\\) still gives us 5 when minimized).\n",
    "\n",
    "$$J(\\theta) = \\sum_{i=1}^m y^{(i)} \\ \\text{cost}_1(\\theta^Tx^{(i)}) + (1 - y^{(i)}) \\ \\text{cost}_0(\\theta^Tx^{(i)}) + \\dfrac{\\lambda}{2}\\sum_{j=1}^n \\Theta^2_j$$\n",
    "\n",
    "Furthermore, convention dictates that we regularize using a factor C, instead of λ, like so:\n",
    "\n",
    "$$J(\\theta) = C\\sum_{i=1}^m y^{(i)} \\ \\text{cost}_1(\\theta^Tx^{(i)}) + (1 - y^{(i)}) \\ \\text{cost}_0(\\theta^Tx^{(i)}) + \\dfrac{1}{2}\\sum_{j=1}^n \\Theta^2_j$$\n",
    "\n",
    "This is equivalent to multiplying the equation by \\\\(C = \\dfrac{1}{\\lambda}\\\\), and thus results in the same values when optimized. Now, when we wish to regularize more (that is, reduce overfitting), we decrease C, and when we wish to regularize less (that is, reduce underfitting), we increase C.\n",
    "\n",
    "Finally, note that the hypothesis of the Support Vector Machine is not interpreted as the probability of y being 1 or 0 (as it is for the hypothesis of logistic regression). Instead, it outputs either 1 or 0. (In technical terms, it is a discriminant function.)\n",
    "\n",
    "$$h_\\theta(x) =\\begin{cases}    1 & \\text{if} \\ \\Theta^Tx \\geq 0 \\\\    0 & \\text{otherwise}\\end{cases}$$\n",
    "\n",
    "\n",
    "## Large Margin Intuition\n",
    "\n",
    "A useful way to think about Support Vector Machines is to think of them as Large Margin Classifiers.\n",
    "\n",
    "If y=1, we want \\\\(\\Theta^Tx \\geq 1\\\\) (not just ≥ 0)\n",
    "\n",
    "If y=0, we want \\\\(\\Theta^Tx \\geq 0\\\\) (not just < 0)\n",
    "\n",
    "Now when we set our constant C to a very **large** value (e.g. 100,000), our optimizing function will constrain Θ such that the equation A (the summation of the cost of each example) equals 0. We impose the following constraints on Θ:\n",
    "\n",
    "\\\\(\\Theta^Tx \\geq 1\\\\) if y=1 and \\\\(\\Theta^Tx \\geq 0\\\\) if y=0.\n",
    "\n",
    "If C is very large, we must choose Θ parameters such that:\n",
    "\n",
    "$$\\sum_{i=1}^m y^{(i)}\\text{cost}_1(\\Theta^Tx) + (1 - y^{(i)})\\text{cost}_0(\\Theta^Tx) = 0$$\n",
    "\n",
    "This reduces our cost function to:\n",
    "\n",
    "$$\\begin{align*}\n",
    "J(\\theta) = C \\cdot 0 + \\dfrac{1}{2}\\sum_{j=1}^n \\Theta^2_j \\newline\n",
    "= \\dfrac{1}{2}\\sum_{j=1}^n \\Theta^2_j\n",
    "\\end{align*}$$\n",
    "\n",
    "Recall the decision boundary from logistic regression (the line separating the positive and negative examples). In SVMs, the decision boundary has the special property that it is **as far away as possible** from both the positive and the negative examples.\n",
    "\n",
    "The distance of the decision boundary to the nearest example is called the **margin**. Since SVMs maximize this margin, it is often called a *Large Margin Classifier*.\n",
    "\n",
    "The SVM will separate the negative and positive examples by a **large margin**.\n",
    "\n",
    "This large margin is only achieved when **C is very large**.\n",
    "\n",
    "Data is **linearly separable** when a **straight line** can separate the positive and negative examples.\n",
    "\n",
    "If we have **outlier** examples that we don't want to affect the decision boundary, then we can **reduce C**.\n",
    "\n",
    "Increasing and decreasing C is similar to respectively decreasing and increasing λ, and can simplify our decision boundary.\n",
    "\n",
    "\n",
    "## Mathematics Behind Large Margin Classification (Optional)\n",
    "\n",
    "### Vector Inner Product\n",
    "\n",
    "Say we have two vectors, u and v:\n",
    "\n",
    "$$\\begin{align*}\n",
    "u = \n",
    "\\begin{bmatrix}\n",
    "u_1 \\newline u_2\n",
    "\\end{bmatrix}\n",
    "& v = \n",
    "\\begin{bmatrix}\n",
    "v_1 \\newline v_2\n",
    "\\end{bmatrix}\n",
    "\\end{align*}$$\n",
    "\n",
    "The **length of vector** v is denoted \\\\(||v||\\\\), and it describes the line on a graph from origin (0,0) to \\\\((v_1,v_2)\\\\).\n",
    "\n",
    "The length of vector v can be calculated with \\\\(\\sqrt{v_1^2 + v_2^2}\\\\) by the Pythagorean theorem.\n",
    "\n",
    "The **projection** of vector v onto vector u is found by taking a right angle from u to the end of v, creating a right triangle.\n",
    "\n",
    "* p= length of projection of v onto the vector u.\n",
    "* u^Tv= p \\cdot ||u||\n",
    "\n",
    "Note that \\\\(u^Tv = ||u|| \\cdot ||v|| \\cos \\theta\\\\) where θ is the angle between u and v. Also, \\\\(p = ||v|| \\cos \\theta\\\\). If you substitute p for \\\\(||v|| \\cos \\theta\\\\), you get \\\\(u^Tv= p \\cdot ||u||\\\\)\n",
    "\n",
    "So that \\\\(u^Tv\\\\) is equal to the length of the projection times the length of vector u.\n",
    "\n",
    "In our example, since u and v are vectors of the same length, \\\\(u^Tv = v^Tu\\\\).\n",
    "\n",
    "$$u^Tv = v^Tu = p \\cdot ||u|| = u_1v_1 + u_2v_2$$\n",
    "\n",
    "If the **angle** between the lines for v and u is **greater than 90 degrees**, then the projection p will be **negative**.\n",
    "\n",
    "$$\\begin{align*}&\\min_\\Theta \\dfrac{1}{2}\\sum_{j=1}^n \\Theta_j^2 \\newline&= \\dfrac{1}{2}(\\Theta_1^2 + \\Theta_2^2 + \\dots + \\Theta_n^2) \\newline&= \\dfrac{1}{2}(\\sqrt{\\Theta_1^2 + \\Theta_2^2 + \\dots + \\Theta_n^2})^2 \\newline&= \\dfrac{1}{2}||\\Theta ||^2 \\newline\\end{align*}$$\n",
    "\n",
    "We can use the same rules to rewrite \\\\(\\Theta^Tx^{(i)}\\\\):\n",
    "\n",
    "$$\\Theta^Tx^{(i)} = p^{(i)} \\cdot ||\\Theta || = \\Theta_1x_1^{(i)} + \\Theta_2x_2^{(i)} + \\dots + \\Theta_n x_n^{(i)}$$\n",
    "\n",
    "So we now have a new **optimization objective** by substituting \\\\(p^{(i)} \\cdot ||\\Theta ||\\\\) for \\\\(\\Theta^Tx^{(i)}\\\\):\n",
    "\n",
    "* If y=1, we want \\\\(p^{(i)} \\cdot ||\\Theta || \\geq 1\\\\)\n",
    "* If y=0, we want \\\\(p^{(i)} \\cdot ||\\Theta || \\leq -1\\\\)\n",
    "    \n",
    "The reason this causes a \"large margin\" is because: the vector for Θ is perpendicular to the decision boundary. In order for our optimization objective (above) to hold true, we need the absolute value of our projections \\\\(p^{(i)}\\\\) to be as large as possible.\n",
    "\n",
    "If \\\\(\\Theta_0 =0\\\\), then all our decision boundaries will intersect (0,0). If \\\\(\\Theta_0 \\neq 0\\\\), the support vector machine will still find a large margin for the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
