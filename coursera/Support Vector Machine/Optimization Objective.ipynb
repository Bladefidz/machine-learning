{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Support Vector Machine (SVM)** is yet another type of supervised machine learning algorithm. It is sometimes cleaner and more powerful.\n",
    "\n",
    "Recall that in logistic regression, we use the following rules:\n",
    "\n",
    "if y=1, then \\\\(h_\\theta(x) \\approx 1\\\\) and \\\\(\\Theta^Tx \\gg 0\\\\)\n",
    "\n",
    "if y=0, then \\\\(h_\\theta(x) \\approx 0\\\\) and \\\\(\\Theta^Tx \\ll 0\\\\)\n",
    "\n",
    "Recall the cost function for (unregularized) logistic regression:\n",
    "\n",
    "$$\\begin{align*}J(\\theta) & = \\frac{1}{m}\\sum_{i=1}^m -y^{(i)} \\log(h_\\theta(x^{(i)})) - (1 - y^{(i)})\\log(1 - h_\\theta(x^{(i)}))\\\\ & = \\frac{1}{m}\\sum_{i=1}^m -y^{(i)} \\log\\Big(\\dfrac{1}{1 + e^{-\\theta^Tx^{(i)}}}\\Big) - (1 - y^{(i)})\\log\\Big(1 - \\dfrac{1}{1 + e^{-\\theta^Tx^{(i)}}}\\Big)\\end{align*}$$\n",
    "\n",
    "To make a support vector machine, we will modify the first term of the cost function \\\\(-\\log(h_{\\theta}(x)) = -\\log\\Big(\\dfrac{1}{1 + e^{-\\theta^Tx}}\\Big)\\\\) so that when \\\\(θ^Tx\\\\) (from now on, we shall refer to this as z) is **greater than** 1, it outputs 0. Furthermore, for values of z less than 1, we shall use a straight decreasing line instead of the sigmoid curve.(In the literature, this is called a hinge loss (https://en.wikipedia.org/wiki/Hinge_loss) function.)\n",
    "\n",
    "![](../img/svm_hing.png)\n",
    "\n",
    "Similarly, we modify the second term of the cost function \\\\(-\\log(1 - h_{\\theta(x)}) = -\\log\\Big(1 - \\dfrac{1}{1 + e^{-\\theta^Tx}}\\Big)\\\\) so that when z is **less than** -1, it outputs 0. We also modify it so that for values of z greater than -1, we use a straight increasing line instead of the sigmoid curve.\n",
    "\n",
    "![](../img/svm_hinge_negative_class.png)\n",
    "\n",
    "We shall denote these as \\\\(cost_1(z)\\\\) and \\\\(cost_0(z)\\\\) (respectively, note that \\\\(cost_1(z)\\\\) is the cost for classifying when y=1, and \\\\(cost_0(z)\\\\) is the cost for classifying when y=0), and we may define them as follows (where k is an arbitrary constant defining the magnitude of the slope of the line):\n",
    "\n",
    "$$z = \\theta^Tx$$\n",
    "$$\\text{cost}_0(z) = \\max(0, k(1+z))$$\n",
    "$$\\text{cost}_1(z) = \\max(0, k(1-z))$$\n",
    "\n",
    "Recall the full cost function from (regularized) logistic regression:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m y^{(i)}(-\\log(h_\\theta(x^{(i)}))) + (1 - y^{(i)})(-\\log(1 - h_\\theta(x^{(i)}))) + \\dfrac{\\lambda}{2m}\\sum_{j=1}^n \\Theta^2_j$$\n",
    "\n",
    "Note that the negative sign has been distributed into the sum in the above equation.\n",
    "\n",
    "We may transform this into the cost function for support vector machines by substituting \\\\(cost_1(z)\\\\) and \\\\(cost_0(z)\\\\):\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m y^{(i)} \\ \\text{cost}_1(\\theta^Tx^{(i)}) + (1 - y^{(i)}) \\ \\text{cost}_0(\\theta^Tx^{(i)}) + \\dfrac{\\lambda}{2m}\\sum_{j=1}^n \\Theta^2_j$$\n",
    "\n",
    "We can optimize this a bit by multiplying this by m (thus removing the m factor in the denominators). Note that this does not affect our optimization, since we're simply multiplying our cost function by a positive constant (for example, minimizing \\\\((u-5)^2 + 1\\\\) gives us 5; multiplying it by 10 to make it \\\\(10(u-5)^2 + 10\\\\) still gives us 5 when minimized).\n",
    "\n",
    "$$J(\\theta) = \\sum_{i=1}^m y^{(i)} \\ \\text{cost}_1(\\theta^Tx^{(i)}) + (1 - y^{(i)}) \\ \\text{cost}_0(\\theta^Tx^{(i)}) + \\dfrac{\\lambda}{2}\\sum_{j=1}^n \\Theta^2_j$$\n",
    "\n",
    "Furthermore, convention dictates that we regularize using a factor C, instead of λ, like so:\n",
    "\n",
    "$$J(\\theta) = C\\sum_{i=1}^m y^{(i)} \\ \\text{cost}_1(\\theta^Tx^{(i)}) + (1 - y^{(i)}) \\ \\text{cost}_0(\\theta^Tx^{(i)}) + \\dfrac{1}{2}\\sum_{j=1}^n \\Theta^2_j$$\n",
    "\n",
    "This is equivalent to multiplying the equation by \\\\(C = \\dfrac{1}{\\lambda}\\\\), and thus results in the same values when optimized. Now, when we wish to regularize more (that is, reduce overfitting), we decrease C, and when we wish to regularize less (that is, reduce underfitting), we increase C.\n",
    "\n",
    "Finally, note that the hypothesis of the Support Vector Machine is not interpreted as the probability of y being 1 or 0 (as it is for the hypothesis of logistic regression). Instead, it outputs either 1 or 0. (In technical terms, it is a discriminant function.)\n",
    "\n",
    "$$h_\\theta(x) =\\begin{cases}    1 & \\text{if} \\ \\Theta^Tx \\geq 0 \\\\    0 & \\text{otherwise}\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
