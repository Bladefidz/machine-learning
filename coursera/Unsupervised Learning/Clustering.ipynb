{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Unsupervised learning is contrasted from supervised learning beacuse it uses an **unlabeled training set** rather than a labeled one.\n",
    "\n",
    "In other words, we don't have the vector y of expected results, we only have a dataset of features where we can find structure.\n",
    "\n",
    "Clustering is good for:\n",
    "* Market segmentation\n",
    "* Social network analysis\n",
    "* Organizing computer clusters\n",
    "* Astronomical data analysis\n",
    "\n",
    "\n",
    "## K-Means Algorithm\n",
    "\n",
    "The K-Mean algorithm is the most popular and widely used algorithm for automatically grouping data into coherent subsets.\n",
    "\n",
    "1. Randomly initialize two points in the dataset called the cluster centroids.\n",
    "2. Cluster assignment: assign all examples into one of two groups based on which cluster centroid the example is closest to.\n",
    "3. Move centroid: compute the averages for all the points inside each of the two cluster centroid groups, then move the cluster centroid points to those averages.\n",
    "4. Re-run (2) and (3) until we have found our clusters.\n",
    "\n",
    "Our main variables are:\n",
    "\n",
    "* K (number of clusters)\n",
    "* Training set \\\\({x^{(1)}, x^{(2)}, \\dots,x^{(m)}}\\\\)\n",
    "* Where \\\\(x^{(i)} \\in \\mathbb{R}^n\\\\)\n",
    "\n",
    "Note that we will not use the \\\\(x_0=1\\\\) convention.\n",
    "\n",
    "**The algorithm**\n",
    "\n",
    "```\n",
    "Randomly initialize K cluster centroids mu(1), mu(2), ..., mu(K)\n",
    "Repeat:\n",
    "   for i = 1 to m:\n",
    "      c(i):= index (from 1 to K) of cluster centroid closest to x(i)\n",
    "   for k = 1 to K:\n",
    "      mu(k):= average (mean) of points assigned to cluster k\n",
    "```\n",
    "\n",
    "The first for-loop is the 'Cluster Assignment' step. We make a vector c where \\\\(c^{(i)}\\\\) represents the centroid assigned to example \\\\(x^{(i)}\\\\).\n",
    "\n",
    "We can write the operation of the Cluster Assignment step more mathematically as follows:\n",
    "\n",
    "$$c^{(i)} = argmin_k\\ ||x^{(i)} - \\mu_k||^2$$\n",
    "\n",
    "That is, each \\\\(c^{(i)}\\\\) contains the index of the centroid that has minimal distance to \\\\(x^{(i)}\\\\).\n",
    "\n",
    "By convention, we square the right-hand-side, which makes the function we are trying to minimize more sharply increasing. It is mostly just a convention. But a convention that helps reduce the computation load because the Euclidean distance requires a square root but it is canceled.\n",
    "\n",
    "Without the square:\n",
    "\n",
    "$$||x^{(i)} - \\mu_k|| = ||\\quad\\sqrt{(x_1^i - \\mu_{1(k)})^2 + (x_2^i - \\mu_{2(k)})^2 + (x_3^i - \\mu_{3(k)})^2 + ...}\\quad||$$\n",
    "\n",
    "With the square:\n",
    "\n",
    "$$||x^{(i)} - \\mu_k||^2 = ||\\quad(x_1^i - \\mu_{1(k)})^2 + (x_2^i - \\mu_{2(k)})^2 + (x_3^i - \\mu_{3(k)})^2 + ...\\quad||$$\n",
    "\n",
    "...so the square convention serves two purposes, minimize more sharply and less computation.\n",
    "\n",
    "The **second for-loop** is the 'Move Centroid' step where we move each centroid to the average of its group.\n",
    "\n",
    "More formally, the equation for this loop is as follows:\n",
    "\n",
    "$$\\mu_k = \\dfrac{1}{n}[x^{(k_1)} + x^{(k_2)} + \\dots + x^{(k_n)}] \\in \\mathbb{R}^n$$\n",
    "\n",
    "Where each of \\\\(x^{(k_1)}, x^{(k_2)}, \\dots, x^{(k_n)}\\\\) are the training examples assigned to group \\\\(mμ_k\\\\).\n",
    "\n",
    "If you have a cluster centroid with **0 points** assigned to it, you can randomly **re-initialize** that centroid to a new point. You can also simply **eliminate** that cluster group.\n",
    "\n",
    "After a number of iterations the algorithm will **converge**, where new iterations do not affect the clusters.\n",
    "\n",
    "Note on non-separated clusters: some datasets have no real inner separation or natural structure. K-means can still evenly segment your data into K subsets, so can still be useful in this case.\n",
    "\n",
    "\n",
    "## Optimization Objective\n",
    "\n",
    "Recall some of the parameters we used in our algorithm:\n",
    "\n",
    "* \\\\(c^{(i)}\\\\) = index of cluster (1,2,...,K) to which example \\\\(x_{(i)}\\\\) is currently assigned\n",
    "* \\\\(\\mu_k\\\\) = cluster centroid k \\\\(μ_k \\in ℝ_n\\\\)\n",
    "* \\\\(\\mu_{c^{(i)}}\\\\)  = cluster centroid of cluster to which example \\\\(x_{(i)}\\\\) has been assigned\n",
    "\n",
    "Using these variables we can define our **cost function**:\n",
    "\n",
    "$$J(c^{(i)},\\dots,c^{(m)},\\mu_1,\\dots,\\mu_K) = \\dfrac{1}{m}\\sum_{i=1}^m ||x^{(i)} - \\mu_{c^{(i)}}||^2$$\n",
    "\n",
    "Our **optimization objective** is to minimize all our parameters using the above cost function:\n",
    "\n",
    "$$min_{c,\\mu}\\ J(c,\\mu)$$\n",
    "\n",
    "That is, we are finding all the values in sets c, representing all our clusters, and μ, representing all our centroids, that will minimize the **average of the distances** of every training example to its corresponding cluster centroid.\n",
    "\n",
    "The above cost function is often called the **distortion** of the training examples.\n",
    "\n",
    "In the cluster **assignment step**, our goal is to:\n",
    "\n",
    "$$Minimize\\ J(…)\\ with\\ c^{(1)},\\dots,c^{(m)}\\ (holding\\ \\mu_1,\\dots,\\mu_K\\ fixed)$$\n",
    "\n",
    "In the **move centroid** step, our goal is to:\n",
    "\n",
    "$$Minimize\\ J(…)\\ with\\ \\mu_1,\\dots,\\mu_K$$\n",
    "\n",
    "With k-means, it is **not possible for the cost function to sometimes increase**. It should always descend.\n",
    "\n",
    "\n",
    "## Random Initialization\n",
    "\n",
    "There's one particular recommended method for randomly initializing your cluster centroids.\n",
    "\n",
    "1. Have K<m. That is, make sure the number of your clusters is less than the number of your training examples.\n",
    "2. Randomly pick K training examples. (Not mentioned in the lecture, but also be sure the selected examples are unique).\n",
    "3. Set \\\\(\\mu_1,\\dots,\\mu_K\\\\) equal to these K examples.\n",
    "\n",
    "K-means **can get stuck in local optima**. To decrease the chance of this happening, you can run the algorithm on many different random initializations. In cases where K<10 it is strongly recommended to run a loop of random initializations.\n",
    "\n",
    "```\n",
    "for i = 1 to 100:\n",
    "   randomly initialize k-means\n",
    "   run k-means to get 'c' and 'm'\n",
    "   compute the cost function (distortion) J(c,m)\n",
    "pick the clustering that gave us the lowest cost\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## Choosing the Number of Clusters\n",
    "\n",
    "Choosing K can be quite arbitrary and ambiguous.\n",
    "\n",
    "**The elbow method**: plot the cost J and the number of clusters K. The cost function should reduce as we increase the number of clusters, and then flatten out. Choose K at the point where the cost function starts to flatten out.\n",
    "\n",
    "However, fairly often, the curve is **very gradual**, so there's no clear elbow.\n",
    "\n",
    "Note: **J will always decrease** as K is increased. The one exception is if k-means gets stuck at a bad local optimum.\n",
    "\n",
    "Another way to choose K is to observe how well k-means performs on a **downstream purpose**. In other words, you choose K that proves to be most useful for some goal you're trying to achieve from using these clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
